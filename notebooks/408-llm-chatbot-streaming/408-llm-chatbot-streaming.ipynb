{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Custom Triton Server Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -f Dockerfile -t triton-server ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download model from Huggingfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "\n",
    "print('Downloading and converting...')\n",
    "ov_config = {'PERFORMANCE_HINT': 'LATENCY', 'NUM_STREAMS': '1'}\n",
    "ov_model = OVModelForCausalLM.from_pretrained(\n",
    "    'togethercomputer/RedPajama-INCITE-Chat-3B-v1',\n",
    "    export=True,\n",
    "    device='CPU',\n",
    "    compile=False,\n",
    "    ov_config=ov_config)\n",
    "\n",
    "print('Saving to ./models ...')\n",
    "ov_model.save_pretrained('./models/')\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -d \\\n",
    "    --shm-size=1g \\\n",
    "    --ulimit memlock=-1 \\\n",
    "    --ulimit stack=67108864 \\\n",
    "    -p 12337:12337 -p 12338:12338 \\\n",
    "    -v $(pwd)/models:/model:rw \\\n",
    "    -v $(pwd)/endpoints:/repository:rw \\\n",
    "    -e http_proxy=$http_proxy \\\n",
    "    -e https_proxy=$https_proxy \\\n",
    "    -e no_proxy=$no_proxy \\\n",
    "    triton-server \\\n",
    "    tritonserver \\\n",
    "        --model-repository /repository \\\n",
    "        --grpc-port 12337 \\\n",
    "        --http-port 12338"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for endpoint to become ready\n",
    "Model loading might take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -i http://localhost:12338/v2/models/llm-streaming/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tritonclient\\[grpc\\] numpy gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import queue\n",
    "\n",
    "import tritonclient.grpc as grpcclient\n",
    "\n",
    "from functools import partial\n",
    "from tritonclient.utils import *\n",
    "\n",
    "\n",
    "TRITON_SERVER_URL = \"localhost:12337\"\n",
    "\n",
    "\n",
    "###### Everything below is red-pajama specific #######\n",
    "start_message = \"\"\n",
    "history_template = \"\\n<human>:{user}\\n<bot>:{assistant}\"\n",
    "current_message_template = \"\\n<human>:{user}\\n<bot>:{assistant}\"\n",
    "\n",
    "def convert_history_to_text(history):\n",
    "    text = start_message + \"\".join([\"\".join([\n",
    "        history_template.format(user=item[0], assistant=item[1])\n",
    "        ]) for item in history[:-1]\n",
    "    ])\n",
    "\n",
    "    text += \"\".join([\"\".join([\n",
    "        current_message_template.format(user=history[-1][0], assistant=history[-1][1])\n",
    "    ])])\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def red_pajama_partial_text_processor(partial_text, new_text):\n",
    "    if new_text == '<':\n",
    "        return partial_text\n",
    "    \n",
    "    partial_text += new_text\n",
    "    return partial_text.split('<bot>:')[-1]\n",
    "################################################\n",
    "\n",
    "\n",
    "def string_to_triton_inputs(payload):\n",
    "    # TODO: Reuse inputs?\n",
    "    inputs = []\n",
    "    inputs.append(grpcclient.InferInput('text', [1], \"BYTES\"))\n",
    "    inputs[0].set_data_from_numpy(np.array([payload], dtype=np.object_))\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def triton_output_to_string(output):\n",
    "    # b'abcd'\n",
    "    return output[0].decode('utf-8', 'ignore')\n",
    "\n",
    "\n",
    "def on_message_recv(message, history):\n",
    "    print('Requesting question:', message)\n",
    "    new_question_with_context = convert_history_to_text(history + [[message, \"\"]])\n",
    "\n",
    "    with grpcclient.InferenceServerClient(url=TRITON_SERVER_URL, verbose=False) as triton_client:\n",
    "        response_queue = queue.Queue()\n",
    "\n",
    "        def callback(response_queue, result, error):\n",
    "            response_queue.put(error if error else result)\n",
    "\n",
    "        triton_client.start_stream(callback=partial(callback, response_queue))\n",
    "        triton_client.async_stream_infer(\n",
    "            model_name='llm-streaming',\n",
    "            inputs=string_to_triton_inputs(new_question_with_context),\n",
    "            request_id=\"0\",  # TODO: Allow multiple chat users\n",
    "            enable_empty_final_response=True)\n",
    "\n",
    "        print(\"Waiting for result...\")\n",
    "        result = \"\"\n",
    "        while True:\n",
    "            response = response_queue.get()\n",
    "            if type(response) == InferenceServerException:\n",
    "                raise response\n",
    "            if response.as_numpy(\"partial_response\") is None:\n",
    "                break\n",
    "            partial_response = triton_output_to_string(response.as_numpy(\"partial_response\"))\n",
    "            print(partial_response, flush=True, end='')\n",
    "            result = red_pajama_partial_text_processor(result, partial_response)\n",
    "            yield result\n",
    "\n",
    "\n",
    "gr.ChatInterface(on_message_recv).queue().launch(server_name=\"0.0.0.0\", server_port=7779)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the chatbot in browser\n",
    "[http://localhost:7779](http://localhost:7779)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
